# udacity-project-cloud-dw
This repository hosts my Udacity's Data Engineering Nanodegree Cloud Data Warehouse Project. If you are interested in knowing more about the project or about the course, please refer to https://www.udacity.com/course/data-engineer-nanodegree--nd027


**Project Description:**

As a data engineer working at a music streaming startup called Sparkify, my job is to create an ETL that gets the data from S3, stages it on staging tables, transforms it to final tables, and stores in a Data Warehouse in AWS Redshift.

There are three original datasets:
a. A sample of the Million Song Dataset. Path: s3://udacity-dend/song_data
b. A log data of customer interaction, generated by an event generator. Path: s3://udacity-dend/log_data
c. Log metadata, with instructions to parse the log JSON file. Path: s3://udacity-dend/log_json_path.json

The expected processed datasets are:

Fact Table:
- songplays - records in event data associated with song plays i.e. records with page NextSong. Columns: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

Dimension Tables:
- users - users in the app. Columns: user_id, first_name, last_name, gender, level
- songs - songs in music database. Columns: song_id, title, artist_id, year, duration
- artists - artists in music database. Columns: artist_id, name, location, latitude, longitude
- time - timestamps of records in songplays broken down into specific units. Columns: start_time, hour, day, week, month, year, weekday

**How to execute the script**

The code in this repository authenticate AWS, creates the tables in Redshift, performatically copies the tables from S3 to staging ones, transforms and inserts the data into the created tables and show some analysis that one could do with this datasets.

To run the code, first run create_tables.py, which creates all the tables from scratch (deleting them if they exist) on AWS redshift. Second, run etl.py, which will perform extract, transform and load on our data. Finally, run tables_analysis.py, which will execute some queries to answer business questions on our data. A printscreen of the results can be seen below.

**Answer of the Analytical Questions:**
![alt text](image.png)